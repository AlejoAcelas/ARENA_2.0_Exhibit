{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym==0.23.1\n",
    "# !pip install pygame\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from typing import Optional, Union, List, Tuple\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import einops\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import gym.envs.registration\n",
    "import gym.spaces\n",
    "\n",
    "Arr = np.ndarray\n",
    "max_episode_steps = 1000\n",
    "N_RUNS = 200\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter2_rl\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"part1_intro_to_rl\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "import part1_intro_to_rl.utils as utils\n",
    "import part1_intro_to_rl.tests as tests\n",
    "from plotly_utils import imshow\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsType = int\n",
    "ActType = int\n",
    "\n",
    "class MultiArmedBandit(gym.Env):\n",
    "    action_space: gym.spaces.Discrete\n",
    "    observation_space: gym.spaces.Discrete\n",
    "    num_arms: int\n",
    "    stationary: bool\n",
    "    arm_reward_means: np.ndarray\n",
    "    arm_star: int\n",
    "\n",
    "    def __init__(self, num_arms=10, stationary=True):\n",
    "        super().__init__()\n",
    "        self.num_arms = num_arms\n",
    "        self.stationary = stationary\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "        self.action_space = gym.spaces.Discrete(num_arms)\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, arm: ActType) -> Tuple[ObsType, float, bool, dict]:\n",
    "        '''\n",
    "        Note: some documentation references a new style which has (termination, truncation) bools in place of the done bool.\n",
    "        '''\n",
    "        assert self.action_space.contains(arm)\n",
    "        if not self.stationary:\n",
    "            q_drift = self.np_random.normal(loc=0.0, scale=0.01, size=self.num_arms)\n",
    "            self.arm_reward_means += q_drift\n",
    "            self.best_arm = int(np.argmax(self.arm_reward_means))\n",
    "        reward = self.np_random.normal(loc=self.arm_reward_means[arm], scale=1.0)\n",
    "        obs = 0\n",
    "        done = False\n",
    "        info = dict(best_arm=self.best_arm)\n",
    "        return (obs, reward, done, info)\n",
    "\n",
    "    def reset(\n",
    "        self, seed: Optional[int] = None, return_info=False, options=None\n",
    "    ) -> Union[ObsType, Tuple[ObsType, dict]]:\n",
    "        super().reset(seed=seed)\n",
    "        if self.stationary:\n",
    "            self.arm_reward_means = self.np_random.normal(loc=0.0, scale=1.0, size=self.num_arms)\n",
    "        else:\n",
    "            self.arm_reward_means = np.zeros(shape=[self.num_arms])\n",
    "        self.best_arm = int(np.argmax(self.arm_reward_means))\n",
    "        if return_info:\n",
    "            return (0, dict())\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        assert mode == \"human\", f\"Mode {mode} not supported!\"\n",
    "        bandit_samples = []\n",
    "        for arm in range(self.action_space.n):\n",
    "            bandit_samples += [np.random.normal(loc=self.arm_reward_means[arm], scale=1.0, size=1000)]\n",
    "        plt.violinplot(bandit_samples, showmeans=True)\n",
    "        plt.xlabel(\"Bandit Arm\")\n",
    "        plt.ylabel(\"Reward Distribution\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our env inside its wrappers looks like: <TimeLimit<OrderEnforcing<MultiArmedBandit<ArmedBanditTestbed-v0>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py:595: UserWarning: \u001b[33mWARN: Overriding environment ArmedBanditTestbed-v0\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {id}\")\n"
     ]
    }
   ],
   "source": [
    "gym.envs.registration.register(\n",
    "    id=\"ArmedBanditTestbed-v0\",\n",
    "    entry_point=MultiArmedBandit,\n",
    "    max_episode_steps=max_episode_steps,\n",
    "    nondeterministic=True,\n",
    "    reward_threshold=1.0,\n",
    "    kwargs={\"num_arms\": 10, \"stationary\": True},\n",
    ")\n",
    "\n",
    "env = gym.make(\"ArmedBanditTestbed-v0\")\n",
    "print(f\"Our env inside its wrappers looks like: {env}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 125.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected correct freq: 0.1, actual: 0.099565\n",
      "Expected average reward: 0.0, actual: 0.006279\n",
      "All tests passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "    Base class for agents in a multi-armed bandit environment\n",
    "\n",
    "    (you do not need to add any implementation here)\n",
    "    '''\n",
    "    rng: np.random.Generator\n",
    "\n",
    "    def __init__(self, num_arms: int, seed: int):\n",
    "        self.num_arms = num_arms\n",
    "        self.reset(seed)\n",
    "\n",
    "    def get_action(self) -> ActType:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def observe(self, action: ActType, reward: float, info: dict) -> None:\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed: int) -> None:\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "\n",
    "def run_episode(env: gym.Env, agent: Agent, seed: int):\n",
    "\n",
    "    (rewards, was_best) = ([], [])\n",
    "\n",
    "    env.reset(seed=seed)\n",
    "    agent.reset(seed=seed)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        arm = agent.get_action()\n",
    "        (obs, reward, done, info) = env.step(arm)\n",
    "        agent.observe(arm, reward, info)\n",
    "        rewards.append(reward)\n",
    "        was_best.append(1 if arm == info[\"best_arm\"] else 0)\n",
    "\n",
    "    rewards = np.array(rewards, dtype=float)\n",
    "    was_best = np.array(was_best, dtype=int)\n",
    "    return (rewards, was_best)\n",
    "\n",
    "\n",
    "def run_agent(env: gym.Env, agent: Agent, n_runs=200, base_seed=1):\n",
    "    all_rewards = []\n",
    "    all_was_bests = []\n",
    "    base_rng = np.random.default_rng(base_seed)\n",
    "    for n in tqdm(range(n_runs)):\n",
    "        seed = base_rng.integers(low=0, high=10_000, size=1).item()\n",
    "        (rewards, corrects) = run_episode(env, agent, seed)\n",
    "        all_rewards.append(rewards)\n",
    "        all_was_bests.append(corrects)\n",
    "    return (np.array(all_rewards), np.array(all_was_bests))\n",
    "\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "\n",
    "    def get_action(self) -> ActType:\n",
    "        return self.rng.integers(low = 0, high = self.num_arms)\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Useful when plotting multiple agents with `plot_rewards`\n",
    "        return \"RandomAgent\"\n",
    "\n",
    "\n",
    "num_arms = 10\n",
    "stationary = True\n",
    "env = gym.make(\"ArmedBanditTestbed-v0\", num_arms=num_arms, stationary=stationary)\n",
    "agent = RandomAgent(num_arms, 0)\n",
    "all_rewards, all_corrects = run_agent(env, agent)\n",
    "\n",
    "print(f\"Expected correct freq: {1/10}, actual: {all_corrects.mean():.6f}\")\n",
    "assert np.isclose(all_corrects.mean(), 1/10, atol=0.05), \"Random agent is not random enough!\"\n",
    "\n",
    "print(f\"Expected average reward: 0.0, actual: {all_rewards.mean():.6f}\")\n",
    "assert np.isclose(all_rewards.mean(), 0, atol=0.05), \"Random agent should be getting mean arm reward, which is zero.\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m optimism \u001b[39min\u001b[39;00m [\u001b[39m0\u001b[39m, \u001b[39m5\u001b[39m]:\n\u001b[1;32m     26\u001b[0m     agent \u001b[39m=\u001b[39m RewardAveraging(num_arms, \u001b[39m0\u001b[39m, epsilon\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, optimism\u001b[39m=\u001b[39moptimism)\n\u001b[0;32m---> 27\u001b[0m     (rewards, num_correct) \u001b[39m=\u001b[39m run_agent(env, agent, n_runs\u001b[39m=\u001b[39;49mN_RUNS, base_seed\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     28\u001b[0m     all_rewards\u001b[39m.\u001b[39mappend(rewards)\n\u001b[1;32m     29\u001b[0m     names\u001b[39m.\u001b[39mappend(\u001b[39mstr\u001b[39m(agent))\n",
      "Cell \u001b[0;32mIn[6], line 49\u001b[0m, in \u001b[0;36mrun_agent\u001b[0;34m(env, agent, n_runs, base_seed)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(n_runs)):\n\u001b[1;32m     48\u001b[0m     seed \u001b[39m=\u001b[39m base_rng\u001b[39m.\u001b[39mintegers(low\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, high\u001b[39m=\u001b[39m\u001b[39m10_000\u001b[39m, size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> 49\u001b[0m     (rewards, corrects) \u001b[39m=\u001b[39m run_episode(env, agent, seed)\n\u001b[1;32m     50\u001b[0m     all_rewards\u001b[39m.\u001b[39mappend(rewards)\n\u001b[1;32m     51\u001b[0m     all_was_bests\u001b[39m.\u001b[39mappend(corrects)\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mrun_episode\u001b[0;34m(env, agent, seed)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m     32\u001b[0m     arm \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mget_action()\n\u001b[0;32m---> 33\u001b[0m     (obs, reward, done, info) \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(arm)\n\u001b[1;32m     34\u001b[0m     agent\u001b[39m.\u001b[39mobserve(arm, reward, info)\n\u001b[1;32m     35\u001b[0m     rewards\u001b[39m.\u001b[39mappend(reward)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/time_limit.py:17\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m---> 17\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/order_enforcing.py:13\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     12\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m observation, reward, done, info\n",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36mMultiArmedBandit.step\u001b[0;34m(self, arm)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, arm: ActType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[ObsType, \u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[1;32m     21\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m    Note: some documentation references a new style which has (termination, truncation) bools in place of the done bool.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mcontains(arm)\n\u001b[1;32m     25\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstationary:\n\u001b[1;32m     26\u001b[0m         q_drift \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnp_random\u001b[39m.\u001b[39mnormal(loc\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, scale\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_arms)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class RewardAveraging(Agent):\n",
    "    def __init__(self, num_arms: int, seed: int, epsilon: float, optimism: float):\n",
    "        super.__init__(num_arms, seed)\n",
    "\n",
    "    def get_action(self):\n",
    "        pass\n",
    "\n",
    "    def observe(self, action, reward, info):\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed: int):\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        # For the legend, when plotting\n",
    "        return f\"RewardAveraging(eps={self.epsilon}, optimism={self.optimism})\"\n",
    "\n",
    "\n",
    "num_arms = 10\n",
    "stationary = True\n",
    "names = []\n",
    "all_rewards = []\n",
    "env = gym.make(\"ArmedBanditTestbed-v0\", num_arms=num_arms, stationary=stationary)\n",
    "\n",
    "for optimism in [0, 5]:\n",
    "    agent = RewardAveraging(num_arms, 0, epsilon=0.01, optimism=optimism)\n",
    "    (rewards, num_correct) = run_agent(env, agent, n_runs=N_RUNS, base_seed=1)\n",
    "    all_rewards.append(rewards)\n",
    "    names.append(str(agent))\n",
    "    print(agent)\n",
    "    print(f\" -> Frequency of correct arm: {num_correct.mean():.4f}\")\n",
    "    print(f\" -> Average reward: {rewards.mean():.4f}\")\n",
    "\n",
    "utils.plot_rewards(all_rewards, names, moving_avg_window=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
