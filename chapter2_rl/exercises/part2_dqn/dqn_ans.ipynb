{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/_functional_pil.py:242: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  interpolation: int = Image.BILINEAR,\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/_functional_pil.py:286: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  interpolation: int = Image.NEAREST,\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/_functional_pil.py:319: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  interpolation: int = Image.BICUBIC,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union, List\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces\n",
    "import gym.envs.registration\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm, trange\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Optional, Union, Tuple\n",
    "import torch as t\n",
    "from torch import nn, Tensor\n",
    "from gym.spaces import Discrete, Box\n",
    "from numpy.random import Generator\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from jaxtyping import Float, Int, Bool\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger, CSVLogger\n",
    "\n",
    "Arr = np.ndarray\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter2_rl\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"part2_dqn\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "from part1_intro_to_rl.utils import make_env\n",
    "from part1_intro_to_rl.solutions import Environment, Toy, Norvig, find_optimal_policy\n",
    "import part2_dqn.utils as utils\n",
    "import part2_dqn.tests as tests\n",
    "from plotly_utils import line, cliffwalk_imshow, plot_cartpole_obs_and_dones\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsType = int\n",
    "ActType = int\n",
    "\n",
    "class DiscreteEnviroGym(gym.Env):\n",
    "    action_space: gym.spaces.Discrete\n",
    "    observation_space: gym.spaces.Discrete\n",
    "\n",
    "    def __init__(self, env: Environment):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.observation_space = gym.spaces.Discrete(env.num_states)\n",
    "        self.action_space = gym.spaces.Discrete(env.num_actions)\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action: ActType) -> Tuple[ObsType, float, bool, dict]:\n",
    "        '''\n",
    "        Samples from the underlying dynamics of the environment\n",
    "        '''\n",
    "        (states, rewards, probs) = self.env.dynamics(self.pos, action)\n",
    "        idx = self.np_random.choice(len(states), p=probs)\n",
    "        (new_state, reward) = (states[idx], rewards[idx])\n",
    "        self.pos = new_state\n",
    "        done = self.pos in self.env.terminal\n",
    "        return (new_state, reward, done, {\"env\": self.env})\n",
    "\n",
    "    def reset(\n",
    "        self, seed: Optional[int] = None, return_info=False, options=None\n",
    "    ) -> Union[ObsType, Tuple[ObsType, dict]]:\n",
    "        super().reset(seed=seed)\n",
    "        self.pos = self.env.start\n",
    "        return (self.pos, {\"env\": self.env}) if return_info else self.pos\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        assert mode == \"human\", f\"Mode {mode} not supported!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registration.register(\n",
    "    id=\"NorvigGrid-v0\",\n",
    "    entry_point=DiscreteEnviroGym,\n",
    "    max_episode_steps=100,\n",
    "    nondeterministic=True,\n",
    "    kwargs={\"env\": Norvig(penalty=-0.04)},\n",
    ")\n",
    "\n",
    "gym.envs.registration.register(\n",
    "    id=\"ToyGym-v0\", \n",
    "    entry_point=DiscreteEnviroGym, \n",
    "    max_episode_steps=2, \n",
    "    nondeterministic=False, \n",
    "    kwargs={\"env\": Toy()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Experience:\n",
    "    '''A class for storing one piece of experience during an episode run'''\n",
    "    obs: ObsType\n",
    "    act: ActType\n",
    "    reward: float\n",
    "    new_obs: ObsType\n",
    "    new_act: Optional[ActType] = None\n",
    "\n",
    "@dataclass\n",
    "class AgentConfig:\n",
    "    '''Hyperparameters for agents'''\n",
    "    epsilon: float = 0.1\n",
    "    lr: float = 0.05\n",
    "    optimism: float = 0\n",
    "\n",
    "\n",
    "defaultConfig = AgentConfig()\n",
    "\n",
    "class Agent:\n",
    "    '''Base class for agents interacting with an environment (you do not need to add any implementation here)'''\n",
    "    rng: np.random.Generator\n",
    "\n",
    "    def __init__(self, env: DiscreteEnviroGym, config: AgentConfig = defaultConfig, gamma: float = 0.99, seed: int = 0):\n",
    "        self.env = env\n",
    "        self.reset(seed)\n",
    "        self.config = config\n",
    "        self.gamma = gamma\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.num_states = env.observation_space.n\n",
    "        self.name = type(self).__name__\n",
    "\n",
    "    def get_action(self, obs: ObsType) -> ActType:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def observe(self, exp: Experience) -> None:\n",
    "        '''\n",
    "        Agent observes experience, and updates model as appropriate.\n",
    "        Implementation depends on type of agent.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed: int) -> None:\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def run_episode(self, seed) -> List[int]:\n",
    "        '''\n",
    "        Simulates one episode of interaction, agent learns as appropriate\n",
    "        Inputs:\n",
    "            seed : Seed for the random number generator\n",
    "        Outputs:\n",
    "            The rewards obtained during the episode\n",
    "        '''\n",
    "        rewards = []\n",
    "        obs = self.env.reset(seed=seed)\n",
    "        self.reset(seed=seed)\n",
    "        done = False\n",
    "        while not done:\n",
    "            act = self.get_action(obs)\n",
    "            (new_obs, reward, done, info) = self.env.step(act)\n",
    "            exp = Experience(obs, act, reward, new_obs)\n",
    "            self.observe(exp)\n",
    "            rewards.append(reward)\n",
    "            obs = new_obs\n",
    "        return rewards\n",
    "\n",
    "    def train(self, n_runs=500):\n",
    "        '''\n",
    "        Run a batch of episodes, and return the total reward obtained per episode\n",
    "        Inputs:\n",
    "            n_runs : The number of episodes to simulate\n",
    "        Outputs:\n",
    "            The discounted sum of rewards obtained for each episode\n",
    "        '''\n",
    "        all_rewards = []\n",
    "        for seed in trange(n_runs):\n",
    "            rewards = self.run_episode(seed)\n",
    "            all_rewards.append(utils.sum_rewards(rewards, self.gamma))\n",
    "        return all_rewards\n",
    "\n",
    "class Random(Agent):\n",
    "    def get_action(self, obs: ObsType) -> ActType:\n",
    "        return self.rng.integers(0, self.num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cheater(Agent):\n",
    "    def __init__(self, env: DiscreteEnviroGym, config: AgentConfig = defaultConfig, gamma=0.99, seed=0):\n",
    "        super().__init__(env, config, gamma, seed)\n",
    "        pass\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        print(self.env.unwrapped.env.R.shape)\n",
    "        \n",
    "        return np.argmax(self.env.unwrapped.env.R[obs].sum(axis = 1), axis = 0)\n",
    "        print(dir(self.env.unwrapped.env))\n",
    "\n",
    "\n",
    "env_toy = gym.make(\"ToyGym-v0\")\n",
    "agents_toy: List[Agent] = [Cheater(env_toy), Random(env_toy)]\n",
    "returns_list = []\n",
    "names_list = []\n",
    "for agent in agents_toy:\n",
    "    returns = agent.train(n_runs=100)\n",
    "    returns_list.append(utils.cummean(returns))\n",
    "    names_list.append(agent.name)\n",
    "\n",
    "line(returns_list, names=names_list, title=f\"Avg. reward on {env_toy.spec.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
